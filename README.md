# CrossSightğŸ‘ï¸ğŸ“ - Image Captioning
A Multimodal Image Captioning System Built From Scratch using PyTorch

CrossSight is an End-to-End Image Captioning System that learns to **see** images and **generates** captions using CNN Encoder and Transformer Based Auto-Regressive Decoder.

The entire architecture is built from scratch using PyTorch , from attention mechanisms - to inference-time decoding , and is Deployed as an interactive web demo using Gradio. 

## DemoğŸ’»: 

HuggingFace Spaces: https://roshan454-crosssight.hf.space/?__theme=system&deep_link=6dMURe2Uxjw

This Demo Generates **token-by-token**  animated output captions which is similar to Large Language models generate text in real time.

(Add Demo_GIF)

## Problem StatementğŸ§ğŸ“: 

Image Captioning is a challenging multi-modal task that requires: 
- Extracting Visual Information from the image.
- Align the Visual Information with language.
- Generate Grammatically Correct and contextually relevant captions.
- Ensuring Autoregressive Generation.

Unlike classification tasks, captioning requires **sequence modeling**, **cross-attention**, and careful handling of training vs inference behavior.

## System ArchitectureğŸ›ï¸ğŸ—ï¸:


![System Architecture](Images_and_Diagrams/CrossSight-System-Architecture_Final.png)

This System follows a CNN Encoder + Transformer Decoder Architecture: 
#### Image Encoder:
- Image is passed to EfficientNetB0 to extract visual features.
- This Features are passed through a linear projection layer which projects to the Transformer's model dimension.
- The Output from the linear layer is treated as Image Memory which is used for **cross-attention** to generate next token.

#### Caption Decoder: 
- The input tokens are passed with token embeddings + positional embeddings.
- Transformer Decoder has Masked Self-Attention and Cross-Attention over the image memory.
- Linear Projection layer to vocabulary logits.

## Model Detailsâš™ï¸ğŸ”: 

#### Encoder: 
- Backbone: EfficientNet-B0(Pre-trained).
- Linear Projection Layer gives output as shape (B,1,512)

#### Decoder: 
- Transformer Decoder(6 layers , 8 heads).
- Uses Causal Masking to prevent seeing the future tokens.
- Cross-Attention with the Image Memory from the Encoder.

## Training ğŸ› ï¸ğŸ”„: 

![Training vs Inference](Images_and_Diagrams/Training_vs._Inference_Final.drawio.png)

- Teacher Forcing is used during Training.
- Captions are shifted:
  - Input: "<START> a little girl riding a bike
  - Output: " a little girl riding a bike <END>" 
- CrossEntropy Loss is computed across all timestep.
- '<PAD>' tokens are ignored during loss computation.

## Inference ğŸ“ğŸ¤”ğŸ’¬:

![Autoregressive Generation](Images_and_Diagrams/Autoregressive_Caption_Generation_Final.drawio.png)

During Inference:
- Generation is started with <START> token.
- Model Predict One Token at a time.
- Each Token is appended to the input sequence.
- Generation stops at <END> or max_length reaches.

This replicates how GPT-style Language models generate text.

## ResultsğŸ“Š:

#### Training Loss CurveğŸ“‰:

![Training_Loss_Curve](Images_and_Diagrams/loss_curve_cross_sight_30_epochs.png)

**ObservationğŸ”¬ğŸ“:**  
- Loss is steadily Decreasing overall.
- Some noise is expected due to small dataset size and Autoregressive sequence learning. 

### Qualitative Example(Seen Data):

![Prediction Example](Images_and_Diagrams/Prediction_Example.png)

**Prediction:**  
> a black dog and a brown dog are standing on the street

**Ground Truth Caption:**  
> a black dog and a white dog with brown spots are staring at each other in the street

**ObservationğŸ”¬ğŸ“:** 
Even when exact wording differs:
- Model correctly identifies the main entities (two dogs)
- the scene (street), and their interaction.  
- shows attributes such as coat color and exact action.

### Qualitative Results(Unseen Data):

To evaluate real-world performance, CrossSight was tested on images **not seen during training**, including samples from the COCO dataset and internet images.

Below are example predictions generated by the deployed Gradio demo.

**Example 1**
![Prediction Example Unseen1](Images_and_Diagrams/unseen_data.png)

**Example 2**
![Prediction Example Unseen2](Images_and_Diagrams/prediction_2.png)

**Example 3**
![Prediction Example Unseen3](Images_and_Diagrams/prediction_3.png)

**Example 4**
![Prediction Example Unseen3](Images_and_Diagrams/prediction_4.png)

### Observations: 

- The model correctly identifies **primary objects** (people, dogs, animals).
- Simple **actions** such as *running*, *playing*, or *holding* are captured well.
- Captions are generally fluent and grammatically correct.

However:
- The model sometimes **misidentifies object types** (e.g., cat â†’ dog).
- Fine-grained distinctions (goat vs dog, baseball vs general play) are challenging.
- This behavior is expected given the limited training data and vocabulary.

## Limitations and Future Improvements: 

### **Limitationsâš ï¸**

1. Single Global Image Representation

  The CNN encoder compresses the entire image into a single vector, which limits fine-grained spatial understanding and object-level reasoning.
  Current: (B, 1, 512)  
  Ideal: (B, N, 512)


2. Limited Training Data

  Training on a relatively small dataset restricts vocabulary richness and complex scene understanding. (Only 8K training images compared to 100M+ training images when SOTA models are trained on)

3. Greedy Decoding

  The model uses greedy decoding, which can lead to:
	â€¢	Less descriptive captions
	â€¢	Missed alternative phrasings
  More advanced decoding strategies can improve caption diversity.

4. Exposure Bias

  The gap between teacher forcing (training) and autoregressive generation (inference) can cause early mistakes to propagate.
  
### **Future ImprovementsğŸ“ˆğŸ”¨:**

This project focus on building from scratch and understanding the entire architecture. Lots of improvements can be made to enhance the performance and scalability.

1. **Patch-Level Image Representation:**
- Instead of projecting the whole image into a single global vector (B,1,512) , We can use encoder to output patch-level embeddings (B,N,512)
- This might improve the cross-attention between more detailed image features and the tokens. 

2. **Advanced Decoding Strategies**
   Instead of Using Greedy Decoding use more improved decoding strategies like top-k , beam-search sampling.

3. **Large Scale Training**
   Training on Larger datasets (e.g. MS COCO , Flickr30k) would:
   - Improve Vocabulary
   - Reduce Object Confusion
   - Enhance Robustness to unseen images

## **Key LearningsğŸ”‘**
- Built a Complete Multimodal system from scratch
- Gained Deep Understanding of Transformer Self-Attention and Cross-Attention
- Learned practical Difference in Training and Inference in Generative Models
- Experienced real-world limitations of small scale model training
- Deployed a complete interactive , user-facing ML system


## Dataset & Training Details ğŸ“¦

- Dataset: Flickr8K
- Training Images: ~8,000
- Captions per Image: Up to 5
- Vocabulary Size: ~8K tokens
- Training Epochs: 30
- Loss Function: CrossEntropyLoss (PAD tokens ignored)
- Optimizer: AdamW

## How to Run Locally ğŸ–¥ï¸

This project is already deployed as a live demo on Hugging Face Spaces.

If youâ€™d like to run the inference demo locally, clone the repository and run the app.  
The pretrained model weights are automatically downloaded from the Hugging Face Model Hub.

```bash
git clone https://github.com/mroshan454/CrossSight_Image_Captioning.git
cd CrossSight_Image_Captioning
pip install -r requirements.txt
python app.py
```

## Why This Project Matters ğŸ¯

Most image captioning projects rely entirely on large pretrained models.

CrossSight was intentionally built **from first principles** to:
- Understand how vision and language interact via cross-attention
- Learn the mechanics of autoregressive sequence generation
- Bridge CNN-based perception with Transformer-based language modeling
- Gain system-level intuition transferable to larger multimodal models

This project prioritizes **architectural understanding over leaderboard performance**.

## Skills Demonstrated ğŸ§ âš™ï¸

- Multimodal deep learning (vision + language)
- Transformer internals (self-attention, cross-attention, masking)
- Autoregressive generation pipelines
- Training vs inference system design
- Model deployment with Gradio & Hugging Face Spaces
- Debugging real-world generalization failures












  





